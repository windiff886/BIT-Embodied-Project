# 复现 2013 DQN（Mnih et al., 2013

> 你已经有了游戏的 `reset/step` 接口。接下来你要做的，就是把它包装成：  
> **（看画面）→（选动作）→（玩一会儿）→（把经历记下来）→（抽记忆来学习）→（变得更会玩）**  
> 这就是 2013 DQN 的完整闭环。

---

## 1. 先搞清楚：DQN 到底在学什么？

### 1.1 它学的是一个“评分器” Q(s, a)

你可以把 DQN 想成：给它一张当前游戏画面（其实是最近几帧叠起来的“短视频”），再给它一个动作，它会输出一个分数：

- **Q(s, a) 越大**：表示“在当前状态 s 做动作 a，以后能拿到的总分（期望）越高”
- 我们要做的是：**训练这个评分器越来越准**
- 当评分器准了，玩游戏时我们就做：  
  **选分数最高的动作**（大部分时间这样做）  
  **偶尔随机试试**（防止陷入局部最优）

---

## 2. 你需要搭的整体系统（6 个部件）

下面 6 个部件缺一不可，按这个顺序实现最稳：

1. **画面预处理**：把游戏画面变成神经网络能吃的固定大小图片  
2. **4 帧叠加成状态**：一张图不够，叠 4 张才知道“球在往哪飞”  
3. **动作重复（frame skip）**：每次选一个动作，让它重复执行 4 帧，省算力更稳  
4. **经验回放（Replay Buffer）**：把经历都存起来，随机抽样来训练  
5. **训练更新（TD 学习）**：用“现在分数 ≈ 奖励 + 未来最高分”这个规则矫正网络  
6. **评估与监控**：定期测分数、看 Q 值是否爆炸

---

## 3. 第一步：画面预处理（Preprocess）

### 3.1 为什么要预处理？
原始画面可能是 RGB、分辨率大、还有计分栏等无关信息。  
DQN 需要统一输入格式，否则难学且耗资源。

### 3.2 你要做什么？
把每一帧画面变成：

- **灰度图（1 通道）**
- **84×84 大小**
- **uint8（0~255）**存起来，节省内存

### 3.3 推荐做法（概念版）
- RGB → 灰度（减少通道）
- resize 到 84×84（或先裁剪后 resize）
- 如果计分栏很大且对控制没用：**裁剪掉**

最终你得到：`frame84`，形状 `(84, 84)`，类型 `uint8`

---

## 4. 第二步：叠 4 帧，构造“状态” phi

### 4.1 为什么要叠 4 帧？
单帧画面不知道速度方向。  
比如 pinball 球的位置只靠一帧很难推“下一秒去哪”。

所以 DQN 用最近 4 帧组成一个状态：

- `phi = [frame(t-3), frame(t-2), frame(t-1), frame(t)]`
- 拼成一个张量：`(84, 84, 4)`

### 4.2 reset 时怎么做？
reset 时只有第一帧，凑不满 4 帧怎么办？

**最常见做法：把第一帧复制 4 份**  
这样第一步也有合法输入。

---

## 5. 第三步：动作重复（Frame Skip / Action Repeat）

### 5.1 “每 4 帧决策一次”是什么意思？
你不需要每一帧都做一次决策，太频繁也更不稳定。  
论文常用：

- 你选一次动作 `a`
- 连续执行 4 次 `env.step(a)`
- 把这 4 次的 reward 加起来当做一次决策的 reward

这叫：
- `action_repeat = 4`

### 5.2 reward 怎么处理？
这一小段（4 次 step）得到的奖励累加成 `R`：

- `R = r1 + r2 + r3 + r4`
- 只取最后一次 step 的画面作为“下一状态的最新帧”

如果中途 `done=True`：
- 立刻停止重复
- 把当前累计奖励 `R` 返回

---

## 6. 第四步：经验回放（Replay Buffer）——“记忆库”

### 6.1 为什么需要 replay？
如果你边玩边立刻用刚刚那一秒的数据训练，会有两个致命问题：

1. **数据高度相关**：连续几秒画面很像，梯度更新会偏
2. **分布在变**：你变聪明了，遇到的状态也变了，训练会震荡

Replay 的作用就是：
- 把经历存起来
- **随机抽一些过去的经历**来训练  
这样更像监督学习：样本独立、分布更平滑。

### 6.2 你到底要存什么？
一条经历（transition）最基础是：

- `state phi_t`
- `action a_t`
- `reward r_t`
- `next_state phi_{t+1}`
- `done`

但为了省内存，推荐你存“单帧”，采样时再拼 4 帧：

**推荐存：**
- `frame_t`：84×84 uint8
- `action_t`：int
- `reward_t`：-1/0/1（裁剪后）
- `done_t`：bool

采样时用索引重建：
- `state` 用 `i-3..i`
- `next_state` 用 `i-2..i+1`

### 6.3 重要坑：不能跨 episode 拼帧
如果 `done=True`，说明游戏结束，下一局的第一帧不能和上一局的最后帧拼在一起。

最简单可靠做法：
- 采样时，如果构造 state 的 4 帧窗口内出现 done，就丢弃这个样本重采

### 6.4 warm-up：先存够再学
刚开始 buffer 里数据太少，会训练崩。

建议：
- 先随机玩一段（例如 50k transitions）把 buffer 填起来
- 再开始训练更新

---

## 7. 第五步：奖励裁剪（非常关键）

### 7.1 为什么要裁剪？
不同游戏奖励尺度差别巨大。Video Pinball 可能一次给很多分，容易把梯度冲爆。

论文做法：
- 正奖励 → +1
- 负奖励 → -1
- 0 → 0

也就是：
- `r_clipped = sign(r_raw)`

注意：这是为了“训练稳定性”，不是为了还原真实得分。

---

## 8. 第六步：训练更新（DQN 的“矫正规则”）

### 8.1 直觉：现在的评分应该等于“马上得分 + 未来最强评分”
对一条经历 `(phi, a, r, phi', done)`：

- 如果游戏结束了（done=True）：
  - 未来没有了，所以目标就是：  
    `target y = r`

- 如果没结束：
  - 未来还能继续玩，我们假设下一步会选“最好的动作”  
  - 所以目标：  
    `y = r + gamma * max_a' Q(phi', a')`

这就是 DQN 的核心：用未来最高 Q 值来构造监督信号。

### 8.2 你训练的 loss 是什么？
你希望网络预测值 `Q(phi, a)` 靠近目标 y：

- `loss = (y - Q(phi,a))^2`
- batch 里平均一下

### 8.3 stop-gradient（非常重要）
`y` 是目标标签，不允许反向传播穿过去，否则会“自己追自己”导致不稳定。

实现上就是：
- 计算 `y` 时禁用梯度（例如 `no_grad`）
- 只对 `Q(phi,a)` 反传更新参数

---

## 9. 探索策略：epsilon-greedy（按论文）

### 9.1 玩的时候怎么选动作？
- 以概率 `epsilon`：随机动作（探索）
- 以概率 `1-epsilon`：选 Q 最大的动作（利用）

### 9.2 epsilon 怎么变化？
论文常用 schedule：

- `epsilon` 从 1.0 线性降到 0.1（前 1,000,000 环境帧）
- 然后固定 0.1

评估时：
- `epsilon = 0.05`

---

## 10. 训练主循环：用“人话”描述每一步

下面是你最终 main loop 在做什么（每一步都是必需的）：

1. reset 游戏，拿到第一帧，预处理成 84×84
2. 复制 4 份构成初始状态 `phi`
3. 重复直到训练结束（比如 10,000,000 帧）：
   1. 按 frame_count 算出当前 `epsilon`
   2. 用 epsilon-greedy 选动作 `a`
   3. 把动作重复执行 4 次，累计奖励 `R`，拿到最后一帧画面
   4. 把累计奖励 `R` 裁剪成 -1/0/1
   5. 预处理最后一帧，更新 4 帧堆叠得到 `phi'`
   6. 把这次经历存进 replay
   7. 如果 replay 已经够大（warm-up 过了）：
      1. 从 replay 随机抽 32 条经历
      2. 算每条的 target y
      3. 算 loss，RMSProp 更新网络
   8. 如果 done：
      - reset，重新初始化 4 帧堆叠

---

## 11. 监控与评估：怎么知道你做对了？

你至少要记录三类指标：

1. **回合得分（episodic return）**
   - 画滑动平均曲线
   - 会很噪，但必须看

2. **Average Q（更平滑）**
   - 先固定收集一批 states（例如随机跑几千步）
   - 定期算 `mean(max_a Q(phi_k,a))`
   - 如果 AvgQ 持续无上限暴涨，通常是训练不稳的信号

3. **loss/TD-error**
   - loss 不一定单调下降，但如果突然爆炸通常是出问题

---

## 12. 经验补丁（可选，但很救命）

2013 版 DQN **没有 target network**，可能在某些环境不稳定。  
如果你忠实复现后出现 Q 爆炸/崩溃，按优先级加：

1. **梯度裁剪**（最小改动）
2. **降低更新频率**（例如每 4 个决策步更新一次）
3. **Target Network**（最有效稳定器，虽是 2015 版，但工程上强推荐）

---

## 13. 最短实现路线（建议照这个顺序写）

1. preprocess + 4 帧堆叠（先跑起来，确保 shape 正确）
2. frame skip wrapper（确保 reward 累加、done 处理正确）
3. replay buffer（确保采样不跨 done）
4. Q 网络 forward（确保输出维度正确）
5. 单步训练更新（确保 loss 可计算、梯度可更新）
6. 完整训练循环 + 日志 + 评估

---

## 14. 你最终要达到的“复现状态”

- 能稳定跑数百万帧不崩
- episodic return 有明显上升趋势（虽然会抖）
- Average Q 曲线平滑上升，不出现无上限爆炸
- 策略可观察到可解释改进（更会挡球/更会控制）

---
# 用“人话”复现 2013 DQN（Mnih et al., 2013）——Video Pinball 版本超详细流程

> 你已经有了 Video Pinball 的 `reset/step` 接口。接下来你要做的，就是把它包装成：  
> **（看画面）→（选动作）→（玩一会儿）→（把经历记下来）→（抽记忆来学习）→（变得更会玩）**  
> 这就是 2013 DQN 的完整闭环。

---

## 1. 先搞清楚：DQN 到底在学什么？

### 1.1 它学的是一个“评分器” Q(s, a)

你可以把 DQN 想成：给它一张当前游戏画面（其实是最近几帧叠起来的“短视频”），再给它一个动作，它会输出一个分数：

- **Q(s, a) 越大**：表示“在当前状态 s 做动作 a，以后能拿到的总分（期望）越高”
- 我们要做的是：**训练这个评分器越来越准**
- 当评分器准了，玩游戏时我们就做：  
  **选分数最高的动作**（大部分时间这样做）  
  **偶尔随机试试**（防止陷入局部最优）

---

## 2. 你需要搭的整体系统（6 个部件）

下面 6 个部件缺一不可，按这个顺序实现最稳：

1. **画面预处理**：把游戏画面变成神经网络能吃的固定大小图片  
2. **4 帧叠加成状态**：一张图不够，叠 4 张才知道“球在往哪飞”  
3. **动作重复（frame skip）**：每次选一个动作，让它重复执行 4 帧，省算力更稳  
4. **经验回放（Replay Buffer）**：把经历都存起来，随机抽样来训练  
5. **训练更新（TD 学习）**：用“现在分数 ≈ 奖励 + 未来最高分”这个规则矫正网络  
6. **评估与监控**：定期测分数、看 Q 值是否爆炸

---

## 3. 第一步：画面预处理（Preprocess）

### 3.1 为什么要预处理？
原始画面可能是 RGB、分辨率大、还有计分栏等无关信息。  
DQN 需要统一输入格式，否则难学且耗资源。

### 3.2 你要做什么？
把每一帧画面变成：

- **灰度图（1 通道）**
- **84×84 大小**
- **uint8（0~255）**存起来，节省内存

### 3.3 推荐做法（概念版）
- RGB → 灰度（减少通道）
- resize 到 84×84（或先裁剪后 resize）
- 如果计分栏很大且对控制没用：**裁剪掉**

最终你得到：`frame84`，形状 `(84, 84)`，类型 `uint8`

---

## 4. 第二步：叠 4 帧，构造“状态” phi

### 4.1 为什么要叠 4 帧？
单帧画面不知道速度方向。  
比如 pinball 球的位置只靠一帧很难推“下一秒去哪”。

所以 DQN 用最近 4 帧组成一个状态：

- `phi = [frame(t-3), frame(t-2), frame(t-1), frame(t)]`
- 拼成一个张量：`(84, 84, 4)`

### 4.2 reset 时怎么做？
reset 时只有第一帧，凑不满 4 帧怎么办？

**最常见做法：把第一帧复制 4 份**  
这样第一步也有合法输入。

---

## 5. 第三步：动作重复（Frame Skip / Action Repeat）

### 5.1 “每 4 帧决策一次”是什么意思？
你不需要每一帧都做一次决策，太频繁也更不稳定。  
论文常用：

- 你选一次动作 `a`
- 连续执行 4 次 `env.step(a)`
- 把这 4 次的 reward 加起来当做一次决策的 reward

这叫：
- `action_repeat = 4`

### 5.2 reward 怎么处理？
这一小段（4 次 step）得到的奖励累加成 `R`：

- `R = r1 + r2 + r3 + r4`
- 只取最后一次 step 的画面作为“下一状态的最新帧”

如果中途 `done=True`：
- 立刻停止重复
- 把当前累计奖励 `R` 返回

---

## 6. 第四步：经验回放（Replay Buffer）——“记忆库”

### 6.1 为什么需要 replay？
如果你边玩边立刻用刚刚那一秒的数据训练，会有两个致命问题：

1. **数据高度相关**：连续几秒画面很像，梯度更新会偏
2. **分布在变**：你变聪明了，遇到的状态也变了，训练会震荡

Replay 的作用就是：
- 把经历存起来
- **随机抽一些过去的经历**来训练  
这样更像监督学习：样本独立、分布更平滑。

### 6.2 你到底要存什么？
一条经历（transition）最基础是：

- `state phi_t`
- `action a_t`
- `reward r_t`
- `next_state phi_{t+1}`
- `done`

但为了省内存，推荐你存“单帧”，采样时再拼 4 帧：

**推荐存：**
- `frame_t`：84×84 uint8
- `action_t`：int
- `reward_t`：-1/0/1（裁剪后）
- `done_t`：bool

采样时用索引重建：
- `state` 用 `i-3..i`
- `next_state` 用 `i-2..i+1`

### 6.3 重要坑：不能跨 episode 拼帧
如果 `done=True`，说明游戏结束，下一局的第一帧不能和上一局的最后帧拼在一起。

最简单可靠做法：
- 采样时，如果构造 state 的 4 帧窗口内出现 done，就丢弃这个样本重采

### 6.4 warm-up：先存够再学
刚开始 buffer 里数据太少，会训练崩。

建议：
- 先随机玩一段（例如 50k transitions）把 buffer 填起来
- 再开始训练更新

---

## 7. 第五步：奖励裁剪（非常关键）

### 7.1 为什么要裁剪？
不同游戏奖励尺度差别巨大。Video Pinball 可能一次给很多分，容易把梯度冲爆。

论文做法：
- 正奖励 → +1
- 负奖励 → -1
- 0 → 0

也就是：
- `r_clipped = sign(r_raw)`

注意：这是为了“训练稳定性”，不是为了还原真实得分。

---

## 8. 第六步：训练更新（DQN 的“矫正规则”）

### 8.1 直觉：现在的评分应该等于“马上得分 + 未来最强评分”
对一条经历 `(phi, a, r, phi', done)`：

- 如果游戏结束了（done=True）：
  - 未来没有了，所以目标就是：  
    `target y = r`

- 如果没结束：
  - 未来还能继续玩，我们假设下一步会选“最好的动作”  
  - 所以目标：  
    `y = r + gamma * max_a' Q(phi', a')`

这就是 DQN 的核心：用未来最高 Q 值来构造监督信号。

### 8.2 你训练的 loss 是什么？
你希望网络预测值 `Q(phi, a)` 靠近目标 y：

- `loss = (y - Q(phi,a))^2`
- batch 里平均一下

### 8.3 stop-gradient（非常重要）
`y` 是目标标签，不允许反向传播穿过去，否则会“自己追自己”导致不稳定。

实现上就是：
- 计算 `y` 时禁用梯度（例如 `no_grad`）
- 只对 `Q(phi,a)` 反传更新参数

---

## 9. 探索策略：epsilon-greedy（按论文）

### 9.1 玩的时候怎么选动作？
- 以概率 `epsilon`：随机动作（探索）
- 以概率 `1-epsilon`：选 Q 最大的动作（利用）

### 9.2 epsilon 怎么变化？
论文常用 schedule：

- `epsilon` 从 1.0 线性降到 0.1（前 1,000,000 环境帧）
- 然后固定 0.1

评估时：
- `epsilon = 0.05`

---

## 10. 训练主循环：用“人话”描述每一步

下面是你最终 main loop 在做什么（每一步都是必需的）：

1. reset 游戏，拿到第一帧，预处理成 84×84
2. 复制 4 份构成初始状态 `phi`
3. 重复直到训练结束（比如 10,000,000 帧）：
   1. 按 frame_count 算出当前 `epsilon`
   2. 用 epsilon-greedy 选动作 `a`
   3. 把动作重复执行 4 次，累计奖励 `R`，拿到最后一帧画面
   4. 把累计奖励 `R` 裁剪成 -1/0/1
   5. 预处理最后一帧，更新 4 帧堆叠得到 `phi'`
   6. 把这次经历存进 replay
   7. 如果 replay 已经够大（warm-up 过了）：
      1. 从 replay 随机抽 32 条经历
      2. 算每条的 target y
      3. 算 loss，RMSProp 更新网络
   8. 如果 done：
      - reset，重新初始化 4 帧堆叠

---

## 11. 监控与评估：怎么知道你做对了？

你至少要记录三类指标：

1. **回合得分（episodic return）**
   - 画滑动平均曲线
   - 会很噪，但必须看

2. **Average Q（更平滑）**
   - 先固定收集一批 states（例如随机跑几千步）
   - 定期算 `mean(max_a Q(phi_k,a))`
   - 如果 AvgQ 持续无上限暴涨，通常是训练不稳的信号

3. **loss/TD-error**
   - loss 不一定单调下降，但如果突然爆炸通常是出问题

---

## 12. 经验补丁（可选，但很救命）

2013 版 DQN **没有 target network**，可能在某些环境不稳定。  
如果你忠实复现后出现 Q 爆炸/崩溃，按优先级加：

1. **梯度裁剪**（最小改动）
2. **降低更新频率**（例如每 4 个决策步更新一次）
3. **Target Network**（最有效稳定器，虽是 2015 版，但工程上强推荐）

---

## 13. 最短实现路线（建议照这个顺序写）

1. preprocess + 4 帧堆叠（先跑起来，确保 shape 正确）
2. frame skip wrapper（确保 reward 累加、done 处理正确）
3. replay buffer（确保采样不跨 done）
4. Q 网络 forward（确保输出维度正确）
5. 单步训练更新（确保 loss 可计算、梯度可更新）
6. 完整训练循环 + 日志 + 评估

---

## 14. 你最终要达到的“复现状态”

- 能稳定跑数百万帧不崩
- episodic return 有明显上升趋势（虽然会抖）
- Average Q 曲线平滑上升，不出现无上限爆炸
- 策略可观察到可解释改进（更会挡球/更会控制）

---
