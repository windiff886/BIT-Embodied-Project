# DQN (2013) 算法详解

本文档从数学原理和代码实现两个角度详细介绍 DQN 2013 算法。

---

## Part I: 数学原理

---

### 1. 强化学习基础

#### 1.1 马尔可夫决策过程 (MDP)

强化学习问题可以形式化为马尔可夫决策过程，定义为五元组 $(S, A, P, R, \gamma)$：

- $S$: 状态空间
- $A$: 动作空间
- $P(s'|s,a)$: 状态转移概率
- $R(s,a,s')$: 奖励函数
- $\gamma \in [0,1]$: 折扣因子

#### 1.2 回报 (Return)

从时刻 $t$ 开始的累积折扣奖励：

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

#### 1.3 价值函数

**状态价值函数** $V^\pi(s)$：从状态 $s$ 出发，遵循策略 $\pi$ 的期望回报

$$
V^\pi(s) = \mathbb{E}_\pi [G_t | S_t = s]
$$

**动作价值函数** $Q^\pi(s,a)$：在状态 $s$ 采取动作 $a$，之后遵循策略 $\pi$ 的期望回报

$$
Q^\pi(s,a) = \mathbb{E}_\pi [G_t | S_t = s, A_t = a]
$$

---

### 2. Q-Learning 算法

#### 2.1 最优动作价值函数

最优策略 $\pi^*$ 对应的动作价值函数：

$$
Q^*(s,a) = \max_\pi Q^\pi(s,a)
$$

#### 2.2 Bellman 最优方程

$Q^*$ 满足 Bellman 最优方程：

$$
Q^*(s,a) = \mathbb{E}_{s'} \left[ r + \gamma \max_{a'} Q^*(s', a') \Big| s, a \right]
$$

#### 2.3 Q-Learning 更新规则

使用时序差分 (TD) 方法迭代更新 Q 值：

$$
Q(s,a) \leftarrow Q(s,a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s,a) \right]
$$

其中：
- $\alpha$：学习率
- $r + \gamma \max_{a'} Q(s', a')$：TD 目标
- $r + \gamma \max_{a'} Q(s', a') - Q(s,a)$：TD 误差

---

### 3. DQN 核心创新

#### 3.1 函数近似

使用神经网络 $Q(s,a;\theta)$ 近似 $Q^*(s,a)$，其中 $\theta$ 为网络参数。

对于 Atari 游戏：
- 输入：$\phi(s) \in \mathbb{R}^{4 \times 84 \times 84}$（4 帧灰度图像堆叠）
- 输出：$Q(\phi(s), a; \theta)$ 对所有动作 $a \in A$

#### 3.2 经验回放 (Experience Replay)

存储经验元组 $(s_t, a_t, r_t, s_{t+1})$ 到回放缓冲区 $D$。

训练时从 $D$ 中均匀随机采样 mini-batch：

$$
(s_j, a_j, r_j, s_{j+1}) \sim \text{Uniform}(D)
$$

**作用**：
1. 打破样本间的时序相关性
2. 提高数据利用效率
3. 平滑数据分布变化

#### 3.3 损失函数

对于 mini-batch 中的每个样本，定义损失：

$$
L(\theta) = \mathbb{E}_{(s,a,r,s') \sim D} \left[ \left( y - Q(s,a;\theta) \right)^2 \right]
$$

其中 TD 目标为：

$$
y = \begin{cases}
r & \text{if episode terminates at } s' \\
r + \gamma \max_{a'} Q(s', a'; \theta) & \text{otherwise}
\end{cases}
$$

#### 3.4 梯度下降

对损失函数求梯度：

$$
\nabla_\theta L(\theta) = \mathbb{E}_{(s,a,r,s') \sim D} \left[ \left( y - Q(s,a;\theta) \right) \nabla_\theta Q(s,a;\theta) \right]
$$

使用 RMSProp 优化器更新参数：

$$
\theta \leftarrow \theta - \alpha \cdot \nabla_\theta L(\theta)
$$

**关键点**：计算 $y$ 时使用 **stop-gradient**，即 $y$ 不参与反向传播。

---

### 4. 探索与利用

#### 4.1 ε-greedy 策略

$$
a = \begin{cases}
\text{random action} & \text{with probability } \epsilon \\
\arg\max_a Q(s,a;\theta) & \text{with probability } 1-\epsilon
\end{cases}
$$

#### 4.2 ε 衰减策略

$$
\epsilon(t) = \begin{cases}
\epsilon_{\text{start}} - \frac{t}{T} (\epsilon_{\text{start}} - \epsilon_{\text{end}}) & \text{if } t < T \\
\epsilon_{\text{end}} & \text{otherwise}
\end{cases}
$$

其中：
- $\epsilon_{\text{start}} = 1.0$
- $\epsilon_{\text{end}} = 0.1$
- $T = 1,000,000$ 帧

---

### 5. 状态表示

#### 5.1 预处理

原始 Atari 帧 $(210 \times 160 \times 3)$ 经过以下处理：

1. **灰度化**：RGB → 灰度，降低维度
2. **下采样**：$210 \times 160 \rightarrow 110 \times 84$
3. **裁剪**：$110 \times 84 \rightarrow 84 \times 84$

#### 5.2 帧堆叠

堆叠最近 4 帧构成状态：

$$
\phi_t = (x_{t-3}, x_{t-2}, x_{t-1}, x_t) \in \mathbb{R}^{4 \times 84 \times 84}
$$

**目的**：捕获运动信息（速度、方向）

#### 5.3 奖励裁剪

$$
r_{\text{clipped}} = \text{sign}(r) = \begin{cases}
+1 & \text{if } r > 0 \\
0 & \text{if } r = 0 \\
-1 & \text{if } r < 0
\end{cases}
$$

**目的**：统一不同游戏的奖励尺度，稳定梯度

---

### 6. 网络结构

卷积神经网络结构：

| 层      | 输入                     | 输出                     | 配置                            |
| ------- | ------------------------ | ------------------------ | ------------------------------- |
| Conv1   | $4 \times 84 \times 84$  | $16 \times 20 \times 20$ | 16 个 $8 \times 8$ 核, stride=4 |
| Conv2   | $16 \times 20 \times 20$ | $32 \times 9 \times 9$   | 32 个 $4 \times 4$ 核, stride=2 |
| Flatten | $32 \times 9 \times 9$   | $2592$                   | —                               |
| FC      | $2592$                   | $256$                    | ReLU                            |
| Output  | $256$                    | $                        | A                               | $ | 线性 |

输出 $Q(s,a;\theta)$ 对应每个动作的 Q 值。

---

### 7. 完整算法

**DQN 算法伪代码：**

```
初始化回放缓冲区 D，容量 N
初始化动作价值函数 Q，随机权重 θ

for episode = 1 to M do
    初始化状态 s₁，预处理得到 φ₁ = φ(s₁)
    
    for t = 1 to T do
        // 选择动作
        以概率 ε 选择随机动作 aₜ
        否则选择 aₜ = argmax_a Q(φₜ, a; θ)
        
        // 执行动作
        执行动作 aₜ，观察奖励 rₜ 和下一状态 sₜ₊₁
        预处理得到 φₜ₊₁ = φ(sₜ₊₁)
        
        // 存储经验
        将 (φₜ, aₜ, rₜ, φₜ₊₁) 存入 D
        
        // 训练
        从 D 中随机采样 mini-batch (φⱼ, aⱼ, rⱼ, φⱼ₊₁)
        
        设置目标：
            yⱼ = rⱼ                           如果 φⱼ₊₁ 是终止状态
            yⱼ = rⱼ + γ max_a' Q(φⱼ₊₁, a'; θ)  否则
        
        对 (yⱼ - Q(φⱼ, aⱼ; θ))² 执行梯度下降
    end for
end for
```

---

## Part II: 代码实现

---

## 1. 项目结构

```
DQN-2013/
├── train.ipynb          # 训练入口 (Jupyter Notebook)
└── dqn/                 # 核心组件包
    ├── __init__.py
    ├── network.py       # Q-Network (CNN)
    ├── agent.py         # DQNAgent (智能体)
    └── replay_buffer.py # Transition + ReplayBuffer
```

---

## 2. 核心组件

### 2.1 Transition（单步经验）

```python
@dataclass
class Transition:
    state: np.ndarray       # (4, 84, 84) 当前状态
    action: int             # 动作索引
    reward: float           # 裁剪后奖励 (-1/0/1)
    next_state: np.ndarray  # (4, 84, 84) 下一状态
    done: bool              # 是否终止
```

### 2.2 ReplayBuffer（经验回放）

- **容量**：100 万条经验
- **功能**：存储 Transition，支持随机采样
- **作用**：打破数据相关性，稳定训练

### 2.3 QNetwork（CNN 网络）

按 2013 论文规范：

| 层    | 配置                     |
| ----- | ------------------------ |
| 输入  | `(batch, 4, 84, 84)`     |
| Conv1 | 16 × 8×8, stride=4, ReLU |
| Conv2 | 32 × 4×4, stride=2, ReLU |
| FC    | 256, ReLU                |
| 输出  | `n_actions` 个 Q 值      |

### 2.4 DQNAgent（智能体）

封装训练逻辑：
- **epsilon-greedy**：探索策略
- **训练更新**：TD 学习 + RMSProp

---

## 3. 训练流程

```
┌─────────────────────────────────────────────────────────────┐
│                       训练主循环                              │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│ 1. 环境初始化                                                │
│    env = VideoPinballEnv()                                  │
│    env = ClipRewardEnv(env)      # 奖励裁剪                  │
│    env = FrameStack(env, 4)      # 4帧堆叠                   │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│ 2. 智能体初始化                                              │
│    agent = DQNAgent(n_actions=9)                            │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
         ┌────────────────────────────────────────┐
         │          主循环 (10M 帧)                 │
         │                                        │
         │  ┌──────────────────────────────────┐  │
         │  │ a. 选动作 (epsilon-greedy)        │  │
         │  │    if rand < epsilon:            │  │
         │  │        随机动作                   │  │
         │  │    else:                         │  │
         │  │        argmax Q(s,a)             │  │
         │  └──────────────────────────────────┘  │
         │                  │                     │
         │                  ▼                     │
         │  ┌──────────────────────────────────┐  │
         │  │ b. 执行动作                       │  │
         │  │    next_state, reward, done      │  │
         │  │    = env.step(action)            │  │
         │  └──────────────────────────────────┘  │
         │                  │                     │
         │                  ▼                     │
         │  ┌──────────────────────────────────┐  │
         │  │ c. 存储经验                       │  │
         │  │    buffer.push(Transition(...))  │  │
         │  └──────────────────────────────────┘  │
         │                  │                     │
         │                  ▼                     │
         │  ┌──────────────────────────────────┐  │
         │  │ d. 训练更新 (warmup 后)           │  │
         │  │    batch = buffer.sample(32)     │  │
         │  │    计算 target: y = r + γ·max Q' │  │
         │  │    loss = MSE(Q(s,a), y)         │  │
         │  │    optimizer.step()              │  │
         │  └──────────────────────────────────┘  │
         │                  │                     │
         │                  ▼                     │
         │  ┌──────────────────────────────────┐  │
         │  │ e. 状态转移                       │  │
         │  │    state = next_state            │  │
         │  │    if done: state = env.reset()  │  │
         │  └──────────────────────────────────┘  │
         │                                        │
         └────────────────────────────────────────┘
```

---

## 4. 关键超参数

| 参数                   | 值        | 说明               |
| ---------------------- | --------- | ------------------ |
| `replay_capacity`      | 1,000,000 | 经验回放容量       |
| `batch_size`           | 32        | 每次训练采样数     |
| `gamma`                | 0.99      | 折扣因子           |
| `learning_rate`        | 0.00025   | RMSProp 学习率     |
| `epsilon_start`        | 1.0       | 初始探索率         |
| `epsilon_end`          | 0.1       | 最终探索率         |
| `epsilon_decay_frames` | 1,000,000 | epsilon 衰减帧数   |
| `warmup_frames`        | 50,000    | 预热帧数（不训练） |

---

## 5. epsilon 衰减策略

```
epsilon
  1.0 ┤■■■■■■
      │      ■■■■
      │          ■■■■
      │              ■■■■
  0.1 ┤                  ■■■■■■■■■■■■■■■■■■■■■
      └─────────────────────────────────────── frames
      0          1M                          10M
```

- 前 100 万帧：从 1.0 线性衰减到 0.1
- 之后：固定 0.1
- 评估时：使用 0.05

---

## 6. TD 学习核心公式

对于一条经验 `(s, a, r, s', done)`：

**目标值计算：**
```
if done:
    y = r
else:
    y = r + γ × max_a' Q(s', a')
```

**损失函数：**
```
L = (y - Q(s, a))²
```

**关键点：**
- `y` 使用 **stop-gradient**（不参与反向传播）
- 只更新 `Q(s, a)` 的网络参数

---

## 7. 运行方式

在 Jupyter Notebook 或 Google Colab 中打开 `train.ipynb` 并运行所有单元格。

日志输出示例：
```
动作空间: 9
设备: cuda
帧: 10,000 | 回合: 12 | 平均奖励: 0.58 | eps: 0.991
帧: 20,000 | 回合: 25 | 平均奖励: 0.62 | eps: 0.982
...
>>> 评估: 1.45
```

---

## 8. 代码对应关系

| 概念       | 文件                   | 类/函数              |
| ---------- | ---------------------- | -------------------- |
| 状态预处理 | `envs/wrappers.py`     | `preprocess_frame()` |
| 帧堆叠     | `envs/wrappers.py`     | `FrameStack`         |
| 奖励裁剪   | `envs/wrappers.py`     | `ClipRewardEnv`      |
| Q 网络     | `dqn/network.py`       | `QNetwork`           |
| 经验回放   | `dqn/replay_buffer.py` | `ReplayBuffer`       |
| 智能体     | `dqn/agent.py`         | `DQNAgent`           |
| 训练循环   | `train.ipynb`          | 训练单元格           |
