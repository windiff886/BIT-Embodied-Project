# Nature DQN (2015) 算法详解

本文档从数学原理和代码实现两个角度详细介绍 Nature DQN 2015 算法。

**论文**：*Human-level control through deep reinforcement learning* (Nature, 2015)

**核心改进**：引入 **Target Network** 解决训练不稳定问题

---

## Part I: 数学原理

---

### 1. DQN 2013 的问题

#### 1.1 训练不稳定的根源

DQN 2013 使用同一个网络同时计算：
1. 当前 Q 值：$Q(s, a; \theta)$
2. TD 目标：$y = r + \gamma \max_{a'} Q(s', a'; \theta)$

**问题**：每次更新 $\theta$ 后，TD 目标 $y$ 也随之改变，导致：

$$
\text{追逐移动目标} \Rightarrow \text{训练震荡} \Rightarrow \text{不收敛}
$$

#### 1.2 直观理解

想象射箭：
- **DQN 2013**：靶子随你射箭而移动 → 难以命中
- **DQN 2015**：靶子固定一段时间 → 容易瞄准

---

### 2. Target Network 核心改进

#### 2.1 双网络结构

Nature DQN 引入两个结构相同但参数不同的网络：

| 网络 | 参数 | 用途 |
|------|------|------|
| Q-Network | $\theta$ | 选择动作，计算当前 Q 值，参与梯度更新 |
| Target Network | $\theta^-$ | 计算 TD 目标，**不参与梯度更新** |

#### 2.2 TD 目标计算

使用 Target Network 计算 TD 目标：

$$
y = \begin{cases}
r & \text{if episode terminates at } s' \\
r + \gamma \max_{a'} Q(s', a'; \theta^-) & \text{otherwise}
\end{cases}
$$

**关键**：$\theta^-$ 是固定的，不随梯度更新而变化。

#### 2.3 参数同步策略

每隔 $C$ 步（如 $C = 10000$），将 Q-Network 参数复制到 Target Network：

$$
\theta^- \leftarrow \theta
$$

同步期间，TD 目标保持稳定，网络可以专注于减小 TD 误差。

#### 2.4 数学分析

**损失函数**（对于一个样本）：

$$
L(\theta) = \left( y - Q(s, a; \theta) \right)^2
$$

**梯度**：

$$
\nabla_\theta L(\theta) = -2 \left( y - Q(s, a; \theta) \right) \nabla_\theta Q(s, a; \theta)
$$

由于 $y$ 由 $\theta^-$ 计算（而非 $\theta$），梯度只流过 $Q(s, a; \theta)$，不会影响目标值。

---

### 3. 网络结构改进

Nature DQN 使用更深更宽的网络：

#### 3.1 网络架构对比

| 层 | DQN 2013 | DQN 2015 (Nature) |
|----|----------|-------------------|
| Conv1 | 16 × 8×8, stride=4 | **32** × 8×8, stride=4 |
| Conv2 | 32 × 4×4, stride=2 | **64** × 4×4, stride=2 |
| Conv3 | — | **64** × 3×3, stride=1 |
| FC | 256 | **512** |
| 参数量 | ~1.7M | ~3.5M |

#### 3.2 特征图尺寸计算

输入：$4 \times 84 \times 84$

| 层 | 输出尺寸 | 计算 |
|----|----------|------|
| Conv1 | $32 \times 20 \times 20$ | $(84-8)/4+1=20$ |
| Conv2 | $64 \times 9 \times 9$ | $(20-4)/2+1=9$ |
| Conv3 | $64 \times 7 \times 7$ | $(9-3)/1+1=7$ |
| Flatten | $3136$ | $64 \times 7 \times 7$ |
| FC | $512$ | — |
| Output | $|A|$ | 动作数量 |

---

### 4. Huber Loss

#### 4.1 MSE 的问题

均方误差对异常值敏感：

$$
L_{\text{MSE}}(\delta) = \delta^2
$$

当 TD 误差 $\delta$ 很大时，梯度 $2\delta$ 也很大，可能导致训练不稳定。

#### 4.2 Huber Loss 定义

Huber Loss（又称 Smooth L1 Loss）：

$$
L_{\text{Huber}}(\delta) = \begin{cases}
\frac{1}{2}\delta^2 & \text{if } |\delta| \leq 1 \\
|\delta| - \frac{1}{2} & \text{otherwise}
\end{cases}
$$

#### 4.3 性质对比

| 误差范围 | MSE 梯度 | Huber 梯度 |
|----------|----------|------------|
| $|\delta| \leq 1$ | $2\delta$ | $\delta$ |
| $|\delta| > 1$ | $2\delta$ (大) | $\pm 1$ (恒定) |

**优势**：
- 小误差区域：二次，快速收敛
- 大误差区域：线性，梯度有界，更鲁棒

---

### 5. 完整算法

**Nature DQN 算法伪代码：**

```
初始化回放缓冲区 D，容量 N
初始化 Q-Network，随机权重 θ
初始化 Target Network，权重 θ⁻ = θ

for episode = 1 to M do
    初始化状态 s₁，预处理得到 φ₁ = φ(s₁)

    for t = 1 to T do
        // 选择动作 (ε-greedy)
        以概率 ε 选择随机动作 aₜ
        否则选择 aₜ = argmax_a Q(φₜ, a; θ)

        // 执行动作
        执行动作 aₜ，观察奖励 rₜ 和下一状态 sₜ₊₁
        预处理得到 φₜ₊₁ = φ(sₜ₊₁)

        // 存储经验
        将 (φₜ, aₜ, rₜ, φₜ₊₁) 存入 D

        // 训练
        从 D 中随机采样 mini-batch (φⱼ, aⱼ, rⱼ, φⱼ₊₁)

        设置目标 (使用 Target Network):
            yⱼ = rⱼ                              如果 φⱼ₊₁ 是终止状态
            yⱼ = rⱼ + γ max_a' Q(φⱼ₊₁, a'; θ⁻)   否则

        对 Huber(yⱼ - Q(φⱼ, aⱼ; θ)) 执行梯度下降，更新 θ

        // 定期同步 Target Network
        每 C 步: θ⁻ ← θ
    end for
end for
```

---

### 6. 与 DQN 2013 的关键差异

| 特性 | DQN 2013 | Nature DQN 2015 |
|------|----------|-----------------|
| Target Network | ❌ 无 | ✅ 有，每 C 步同步 |
| TD 目标计算 | $Q(s', a'; \theta)$ | $Q(s', a'; \theta^-)$ |
| 网络深度 | 2 层卷积 | 3 层卷积 |
| 隐藏单元 | 256 | 512 |
| 损失函数 | MSE | Huber Loss |
| 梯度裁剪 | 无 | 有 |
| 训练稳定性 | 较差 | 显著提升 |

---

## Part II: 代码实现

---

## 1. 项目结构

```
DQN-2015/
├── train.ipynb          # 训练入口 (Jupyter Notebook)
└── dqn/                 # 核心组件包
    ├── __init__.py
    ├── network.py       # Q-Network (更深的 CNN)
    ├── agent.py         # DQNAgent (含 Target Network)
    └── replay_buffer.py # Transition + ReplayBuffer
```

---

## 2. 核心组件

### 2.1 QNetwork（改进的 CNN）

按 Nature 2015 论文规范：

| 层 | 配置 |
|----|------|
| 输入 | `(batch, 4, 84, 84)` |
| Conv1 | 32 × 8×8, stride=4, ReLU |
| Conv2 | 64 × 4×4, stride=2, ReLU |
| Conv3 | 64 × 3×3, stride=1, ReLU |
| Flatten | 3136 |
| FC | 512, ReLU |
| 输出 | `n_actions` 个 Q 值 |

**代码实现** (`dqn/network.py`)：

```python
class QNetwork(nn.Module):
    def __init__(self, n_actions: int):
        super(QNetwork, self).__init__()

        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)

        self.fc1 = nn.Linear(64 * 7 * 7, 512)
        self.fc2 = nn.Linear(512, n_actions)

    def forward(self, x):
        if x.dtype == torch.uint8:
            x = x.float() / 255.0

        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        return self.fc2(x)
```

### 2.2 DQNAgent（含 Target Network）

**关键改进**：
- 维护两个网络：`q_network` 和 `target_network`
- 使用 `target_network` 计算 TD 目标
- 定期同步参数

**代码实现** (`dqn/agent.py`)：

```python
@dataclass
class DQNAgent:
    n_actions: int
    target_update_freq: int = 10_000  # Target Network 同步频率

    def __post_init__(self):
        # 主网络
        self.q_network = QNetwork(self.n_actions).to(self.device)

        # Target Network (结构相同，参数独立)
        self.target_network = QNetwork(self.n_actions).to(self.device)
        self.target_network.load_state_dict(self.q_network.state_dict())
        self.target_network.eval()  # 不需要梯度

    def sync_target_network(self):
        """将主网络参数同步到目标网络"""
        self.target_network.load_state_dict(self.q_network.state_dict())
```

### 2.3 训练更新（核心差异）

**DQN 2013 的 TD 目标**：
```python
# 使用同一个网络
next_q = self.q_network(next_states)
target_q = rewards + gamma * next_q.max(dim=1)[0] * (1 - dones)
```

**Nature DQN 2015 的 TD 目标**：
```python
# 使用 Target Network
with torch.no_grad():
    next_q = self.target_network(next_states)  # 关键改动
    target_q = rewards + gamma * next_q.max(dim=1)[0] * (1 - dones)
```

**完整训练步骤**：

```python
def train_step(self):
    # 采样
    transitions = self.replay_buffer.sample(self.batch_size)

    # 准备数据
    states = torch.from_numpy(np.stack([t.state for t in transitions]))
    actions = torch.tensor([t.action for t in transitions])
    rewards = torch.tensor([t.reward for t in transitions])
    next_states = torch.from_numpy(np.stack([t.next_state for t in transitions]))
    dones = torch.tensor([t.done for t in transitions])

    # 当前 Q 值
    current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)

    # TD 目标 (使用 Target Network)
    with torch.no_grad():
        next_q = self.target_network(next_states)
        max_next_q = next_q.max(dim=1)[0]
        target_q = rewards + self.gamma * max_next_q * (1 - dones)

    # Huber Loss
    loss = F.smooth_l1_loss(current_q, target_q)

    # 梯度下降
    self.optimizer.zero_grad()
    loss.backward()
    nn.utils.clip_grad_norm_(self.q_network.parameters(), self.grad_clip)
    self.optimizer.step()

    # 定期同步 Target Network
    if self.frame_count % self.target_update_freq == 0:
        self.sync_target_network()

    return loss.item()
```

---

## 3. 训练流程

```
┌─────────────────────────────────────────────────────────────┐
│                     Nature DQN 训练流程                       │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│ 1. 初始化                                                    │
│    q_network: 随机权重 θ                                     │
│    target_network: 复制 θ⁻ = θ                               │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
         ┌────────────────────────────────────────┐
         │          主循环 (30M 帧)                 │
         │                                        │
         │  ┌──────────────────────────────────┐  │
         │  │ a. 选动作 (ε-greedy)              │  │
         │  │    使用 q_network 选择动作        │  │
         │  └──────────────────────────────────┘  │
         │                  │                     │
         │                  ▼                     │
         │  ┌──────────────────────────────────┐  │
         │  │ b. 执行动作，存储经验              │  │
         │  └──────────────────────────────────┘  │
         │                  │                     │
         │                  ▼                     │
         │  ┌──────────────────────────────────┐  │
         │  │ c. 计算 TD 目标                   │  │
         │  │    y = r + γ·max Q(s'; θ⁻)       │  │
         │  │    ↑ 使用 target_network         │  │
         │  └──────────────────────────────────┘  │
         │                  │                     │
         │                  ▼                     │
         │  ┌──────────────────────────────────┐  │
         │  │ d. 更新 q_network                 │  │
         │  │    loss = Huber(y - Q(s,a; θ))   │  │
         │  │    θ ← θ - α·∇loss               │  │
         │  └──────────────────────────────────┘  │
         │                  │                     │
         │                  ▼                     │
         │  ┌──────────────────────────────────┐  │
         │  │ e. 每 C 步同步                    │  │
         │  │    θ⁻ ← θ                        │  │
         │  └──────────────────────────────────┘  │
         │                                        │
         └────────────────────────────────────────┘
```

---

## 4. 关键超参数

| 参数 | 值 | 说明 |
|------|-----|------|
| `replay_capacity` | 1,000,000 | 经验回放容量 |
| `batch_size` | 32 | 每次训练采样数 |
| `gamma` | 0.99 | 折扣因子 |
| `learning_rate` | 0.00025 | RMSProp 学习率 |
| `epsilon_start` | 1.0 | 初始探索率 |
| `epsilon_end` | 0.1 | 最终探索率 |
| `epsilon_decay_frames` | 1,000,000 | ε 衰减帧数 |
| `warmup_frames` | 50,000 | 预热帧数 |
| `target_update_freq` | 10,000 | Target Network 同步频率 |
| `grad_clip` | 10.0 | 梯度裁剪阈值 |

---

## 5. Target Network 同步可视化

```
θ (Q-Network)      θ⁻ (Target Network)
    │                     │
    │  更新               │  固定
    ▼                     │
   step 1                 │
   step 2                 │
   ...                    │
   step C  ─────复制────→ │  θ⁻ ← θ
    │                     │
   step C+1               │  固定
   step C+2               │
   ...                    │
   step 2C ─────复制────→ │  θ⁻ ← θ
    │                     │
   ...                   ...
```

**效果**：
- TD 目标在 C 步内保持稳定
- 避免"追逐移动目标"问题
- 显著提升训练稳定性

---

## 6. 代码对应关系

| 概念 | 文件 | 类/函数 |
|------|------|---------|
| 状态预处理 | `envs/wrappers.py` | `preprocess_frame()` |
| 帧堆叠 | `envs/wrappers.py` | `FrameStack` |
| 奖励裁剪 | `envs/wrappers.py` | `ClipRewardEnv` |
| Q 网络 | `dqn/network.py` | `QNetwork` |
| 经验回放 | `dqn/replay_buffer.py` | `ReplayBuffer` |
| 智能体 | `dqn/agent.py` | `DQNAgent` |
| Target 同步 | `dqn/agent.py` | `sync_target_network()` |
| 训练循环 | `train.ipynb` | 训练单元格 |

---

## 7. 运行方式

在 Jupyter Notebook 或 Google Colab 中打开 `DQN-2015/train.ipynb` 并运行所有单元格。

日志输出示例：
```
动作空间: 4
并行环境数: 4
设备: cuda
Target Network 同步频率: 10000
帧: 50,000 | 回合: 120 | 平均奖励: 1.25 | eps: 0.950
帧: 100,000 | 回合: 245 | 平均奖励: 2.18 | eps: 0.900
...
```

---

## 8. 保存和加载

Nature DQN 需要保存两个网络的参数：

```python
# 保存
torch.save({
    'q_network': agent.q_network.state_dict(),
    'target_network': agent.target_network.state_dict(),
    'optimizer': agent.optimizer.state_dict(),
    'frame_count': agent.frame_count,
}, 'checkpoint.pt')

# 加载
checkpoint = torch.load('checkpoint.pt')
agent.q_network.load_state_dict(checkpoint['q_network'])
agent.target_network.load_state_dict(checkpoint['target_network'])
agent.optimizer.load_state_dict(checkpoint['optimizer'])
agent.frame_count = checkpoint['frame_count']
```

---

## 9. 总结

Nature DQN 2015 通过 **Target Network** 解决了 DQN 2013 训练不稳定的核心问题：

| 改进 | 作用 |
|------|------|
| Target Network | TD 目标稳定，避免震荡 |
| 定期同步 | 平衡稳定性和学习效率 |
| 更深网络 | 更强的特征提取能力 |
| Huber Loss | 对异常值更鲁棒 |
| 梯度裁剪 | 防止梯度爆炸 |

这些改进使 DQN 达到了人类水平的 Atari 游戏表现，成为深度强化学习的里程碑。
