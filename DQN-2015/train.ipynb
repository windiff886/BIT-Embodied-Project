{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nature DQN (2015) 训练 Notebook\n",
    "\n",
    "本 Notebook 用于训练 Nature DQN 智能体玩 Atari 游戏。\n",
    "\n",
    "**2015 版本核心改进：**\n",
    "- **Target Network**: 使用独立的目标网络计算 TD 目标\n",
    "- **定期同步**: 每 10,000 步将主网络参数复制到目标网络\n",
    "- **更大的网络**: 32-64-64 卷积层 + 512 全连接层\n",
    "- **Huber Loss**: 比 MSE 更鲁棒"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 导入到 Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 挂载 Google Drive 保存检查点\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "%cd /content/drive/MyDrive/Code/BIT-Embodied-Project/DQN-2015\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T05:49:31.984992177Z",
     "start_time": "2026-01-16T05:49:30.869586189Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# 自动检测设备（支持 CUDA、MPS、CPU）\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'\n",
    "\n",
    "DEVICE = get_device()\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"使用设备: {DEVICE}\")\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    print(f\"CUDA: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU 内存: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "elif DEVICE == 'cpu':\n",
    "    print(\"注意: 使用 CPU 训练会很慢，建议减少 total_frames 或使用 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T05:50:38.642487776Z",
     "start_time": "2026-01-16T05:50:38.492788569Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import time\n",
    "import imageio\n",
    "\n",
    "from envs import BreakoutEnv\n",
    "from envs.wrappers import FrameStack, ClipRewardEnv  \n",
    "from dqn import DQNAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 训练配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T05:50:41.158494838Z",
     "start_time": "2026-01-16T05:50:41.125913551Z"
    }
   },
   "outputs": [],
   "source": [
    "# 训练参数\n",
    "CONFIG = {\n",
    "    'total_frames': 50_000_000,     # 总训练帧数\n",
    "    'log_interval': 10_000,          # 日志输出间隔\n",
    "    'plot_interval': 50_000,         # 绘图更新间隔\n",
    "    'eval_interval': 100_000,        # 评估间隔\n",
    "    'save_interval': 500_000,        # 保存间隔\n",
    "    'save_dir': 'checkpoints',       # 保存目录\n",
    "}\n",
    "\n",
    "print(\"训练配置:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 初始化环境和智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T05:50:47.323952057Z",
     "start_time": "2026-01-16T05:50:45.334736336Z"
    }
   },
   "outputs": [],
   "source": [
    "from gymnasium.vector import AsyncVectorEnv\n",
    "import multiprocessing as mp\n",
    "\n",
    "# 使用 spawn 方式创建子进程（避免 CUDA fork 问题）\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "# 并行环境配置（CPU 模式下建议减少并行数）\n",
    "NUM_ENVS = 8 if DEVICE == 'cuda' else 4\n",
    "\n",
    "# 环境工厂函数\n",
    "def make_env():\n",
    "    env = BreakoutEnv()\n",
    "    env = ClipRewardEnv(env)\n",
    "    env = FrameStack(env, num_stack=4)\n",
    "    return env\n",
    "\n",
    "# 创建异步并行环境（真正的多进程并行）\n",
    "envs = AsyncVectorEnv(\n",
    "    [make_env for _ in range(NUM_ENVS)],\n",
    "    shared_memory=True,  # 使用共享内存加速数据传输\n",
    ")\n",
    "\n",
    "# 获取动作空间\n",
    "single_env = make_env()\n",
    "n_actions = single_env.get_action_space()\n",
    "single_env.close()\n",
    "print(f\"动作空间: {n_actions}\")\n",
    "print(f\"并行环境数: {NUM_ENVS} (AsyncVectorEnv)\")\n",
    "\n",
    "# 创建 Nature DQN 智能体 (带 Target Network)\n",
    "agent = DQNAgent(\n",
    "    n_actions=n_actions,\n",
    "    num_envs=NUM_ENVS,      # 传递并行环境数量\n",
    "    device=DEVICE,          # 使用自动检测的设备\n",
    "    target_update_freq=10_000,  # 每 10000 步同步目标网络\n",
    "    use_compile=(DEVICE == 'cuda')  # 仅 CUDA 模式下使用编译加速\n",
    ")\n",
    "print(f\"设备: {agent.device}\")\n",
    "print(f\"Target Network 同步频率: {agent.target_update_freq}\")\n",
    "\n",
    "os.makedirs(CONFIG['save_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 定义绘图函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T05:51:42.421348041Z",
     "start_time": "2026-01-16T05:51:42.410285579Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_progress(episode_rewards, losses, frame):\n",
    "    \"\"\"绘制训练进度\"\"\"\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    \n",
    "    # 奖励曲线\n",
    "    ax1 = axes[0]\n",
    "    if len(episode_rewards) > 0:\n",
    "        ax1.plot(episode_rewards, alpha=0.3, color='blue', label='Episode')\n",
    "        if len(episode_rewards) >= 100:\n",
    "            window = 100\n",
    "            avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\n",
    "            ax1.plot(range(window-1, len(episode_rewards)), avg, color='red', label=f'{window}-ep avg')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Reward')\n",
    "    ax1.set_title(f'Nature DQN (2015) - Frame: {frame:,}')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 损失曲线\n",
    "    ax2 = axes[1]\n",
    "    if len(losses) > 0:\n",
    "        ax2.plot(losses, alpha=0.5, color='green')\n",
    "    ax2.set_xlabel('Update Steps')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('Huber Loss')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T08:10:42.442256362Z",
     "start_time": "2026-01-16T05:51:44.451675312Z"
    }
   },
   "outputs": [],
   "source": [
    "# 统计数据\n",
    "episode_rewards = []  # 裁剪后的奖励 (用于训练)\n",
    "episode_raw_rewards = []  # 原始游戏分数 (用于日志显示)\n",
    "losses = []\n",
    "episode_rewards_buffer = [0.0] * NUM_ENVS\n",
    "episode_raw_rewards_buffer = [0.0] * NUM_ENVS  # 原始分数缓冲\n",
    "episode_count = 0\n",
    "best_avg_reward = float('-inf')  # 记录最佳平均奖励\n",
    "states, _ = envs.reset()\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"开始训练 Nature DQN (2015)...\")\n",
    "\n",
    "for step in range(1, CONFIG['total_frames'] // NUM_ENVS + 1):\n",
    "    frame = step * NUM_ENVS\n",
    "    agent.frame_count = frame\n",
    "    \n",
    "    # 批量选择动作\n",
    "    actions = agent.select_actions_batch(states)\n",
    "    \n",
    "    # 执行动作\n",
    "    next_states, rewards, terminateds, truncateds, infos = envs.step(actions)\n",
    "    dones = terminateds | truncateds\n",
    "    \n",
    "    # 为每个环境存储单帧经验（内存优化：只存最新帧）\n",
    "    for i in range(NUM_ENVS):\n",
    "        current_frame = states[i][-1]  # 取堆叠状态的最新帧 (84, 84)\n",
    "        agent.replay_buffer.push(current_frame, actions[i], rewards[i], dones[i], env_id=i)\n",
    "        episode_rewards_buffer[i] += rewards[i]\n",
    "        \n",
    "        # 从 info 中获取原始分数 (AsyncVectorEnv 返回的 infos 是字典)\n",
    "        raw_reward = infos.get('raw_reward', rewards)[i] if 'raw_reward' in infos else rewards[i]\n",
    "        episode_raw_rewards_buffer[i] += raw_reward\n",
    "        \n",
    "        if dones[i]:\n",
    "            episode_rewards.append(episode_rewards_buffer[i])\n",
    "            episode_raw_rewards.append(episode_raw_rewards_buffer[i])\n",
    "            episode_count += 1\n",
    "            episode_rewards_buffer[i] = 0.0\n",
    "            episode_raw_rewards_buffer[i] = 0.0\n",
    "    \n",
    "    # 训练更新 (内部会自动同步 target network)\n",
    "    loss = agent.update()\n",
    "    if loss is not None:\n",
    "        losses.append(loss)\n",
    "    \n",
    "    states = next_states\n",
    "    \n",
    "    # 日志\n",
    "    if frame % CONFIG['plot_interval'] == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        fps = frame / elapsed\n",
    "        plot_progress(episode_rewards, losses[-1000:], frame)\n",
    "        \n",
    "        current_avg = np.mean(episode_rewards[-100:]) if episode_rewards else 0\n",
    "        current_raw_avg = np.mean(episode_raw_rewards[-100:]) if episode_raw_rewards else 0\n",
    "        print(f\"帧: {frame:,} | 回合: {episode_count} | \"\n",
    "              f\"平均奖励(100): {current_avg:.2f} | \"\n",
    "              f\"原始分数(100): {current_raw_avg:.2f} | \"\n",
    "              f\"epsilon: {agent.get_epsilon():.3f} | FPS: {fps:.0f}\")\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if len(episode_rewards) >= 100 and current_avg > best_avg_reward:\n",
    "            best_avg_reward = current_avg\n",
    "            agent.save(os.path.join(CONFIG['save_dir'], 'dqn_best.pt'))\n",
    "            print(f\"  -> 新最高分！已保存模型 (avg: {best_avg_reward:.2f})\")\n",
    "\n",
    "envs.close()\n",
    "print(\"\\n训练完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 查看最终结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最终训练曲线\n",
    "plot_progress(episode_rewards, losses[-1000:], CONFIG['total_frames'])\n",
    "\n",
    "print(f\"\\n训练统计:\")\n",
    "print(f\"  总回合数: {len(episode_rewards)}\")\n",
    "print(f\"  最高奖励: {max(episode_rewards) if episode_rewards else 0:.2f}\")\n",
    "print(f\"  最终 100 回合平均: {np.mean(episode_rewards[-100:]) if episode_rewards else 0:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 评估训练好的智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, n_episodes=10, save_video=True, video_dir='videos'):\n",
    "    \"\"\"评估智能体，可选保存视频\"\"\"\n",
    "    eval_env = BreakoutEnv(render_mode='rgb_array' if save_video else None)\n",
    "    eval_env = ClipRewardEnv(eval_env)\n",
    "    eval_env = FrameStack(eval_env, num_stack=4)\n",
    "    \n",
    "    if save_video:\n",
    "        os.makedirs(video_dir, exist_ok=True)\n",
    "    \n",
    "    rewards = []\n",
    "    for ep in range(n_episodes):\n",
    "        state, _ = eval_env.reset()\n",
    "        total = 0\n",
    "        done = False\n",
    "        frames = []\n",
    "        \n",
    "        while not done:\n",
    "            if save_video:\n",
    "                frames.append(eval_env.unwrapped.ale.getScreenRGB())\n",
    "            \n",
    "            action = agent.select_action(state, eval_mode=True)\n",
    "            state, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total += reward\n",
    "        \n",
    "        rewards.append(total)\n",
    "        print(f\"  回合 {ep+1}: {total:.2f}\")\n",
    "        \n",
    "        if save_video and frames:\n",
    "            video_path = os.path.join(video_dir, f'episode_{ep+1}.mp4')\n",
    "            imageio.mimsave(video_path, frames, fps=30, macro_block_size=1)\n",
    "            print(f\"    视频已保存: {video_path}\")\n",
    "    \n",
    "    eval_env.close()\n",
    "    return np.mean(rewards), np.std(rewards)\n",
    "\n",
    "print(\"评估中...\")\n",
    "mean_reward, std_reward = evaluate(agent, n_episodes=10, save_video=True)\n",
    "print(f\"\\n评估结果: {mean_reward:.2f} ± {std_reward:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
